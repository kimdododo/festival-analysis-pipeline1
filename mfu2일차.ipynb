{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODUIsdHtggMlJcgf2Ed6nb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdododo/festival-analysis-pipeline1/blob/main/mfu2%EC%9D%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPhYhMMkGtDl",
        "outputId": "505883be-ef07-45d8-a915-f2991a4e9a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from thop) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->thop) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->thop) (3.0.3)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.15.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=94b2b8247383dc8189ecc832b4f317ce25fa12856e508a15aded85f066818b7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=1aca5131629c7c204a67cbff3d6cb1b34047d4e4a079b67ad5dbf1940b6322b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ptflops) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.3)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.5\n",
            "Collecting torchprofile\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.4->torchprofile) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->torchprofile) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->torchprofile) (3.0.3)\n",
            "Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: torchprofile\n",
            "Successfully installed torchprofile-0.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install thop\n",
        "!pip install fvcore\n",
        "!pip install ptflops\n",
        "!pip install torchprofile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "cpsIEGjpGw0N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpu_info():\n",
        "    \"\"\"GPU ì •ë³´ ë° ì´ë¡ ì  ìµœëŒ€ FLOPS í™•ì¸\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "        # GPUë³„ ì´ë¡ ì  TFLOPS (ëŒ€ëµì ì¸ ê°’)\n",
        "        theoretical_tflops = {\n",
        "            'A100': 312,  # FP16 Tensor Core\n",
        "            'V100': 125,  # FP16 Tensor Core\n",
        "            'T4': 65,     # FP16 Tensor Core\n",
        "            'P100': 21.2, # FP16\n",
        "        }\n",
        "\n",
        "        print(f\"GPU: {gpu_name}\")\n",
        "        print(f\"Memory: {gpu_memory:.2f} GB\")\n",
        "\n",
        "        for gpu_model, tflops in theoretical_tflops.items():\n",
        "            if gpu_model in gpu_name:\n",
        "                print(f\"Theoretical Peak Performance: {tflops} TFLOPS (FP16)\")\n",
        "                return tflops * 1e12  # Convert to FLOPS\n",
        "\n",
        "    return None\n",
        "\n",
        "peak_flops = get_gpu_info()\n",
        "peak_flops"
      ],
      "metadata": {
        "id": "AsbHrtTyG1MO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FLOPsCalculator:\n",
        "    \"\"\"ê° ë ˆì´ì–´ë³„ FLOPsë¥¼ ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d_flops(in_channels, out_channels, kernel_size, input_size, stride=1, padding=0):\n",
        "        \"\"\"Conv2D ë ˆì´ì–´ì˜ FLOPs ê³„ì‚°\n",
        "\n",
        "        FLOPs = 2 Ã— KÂ² Ã— C_in Ã— C_out Ã— H_out Ã— W_out\n",
        "        \"\"\"\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "        if isinstance(stride, int):\n",
        "            stride = (stride, stride)\n",
        "        if isinstance(padding, int):\n",
        "            padding = (padding, padding)\n",
        "\n",
        "        h_out = (input_size[0] + 2 * padding[0] - kernel_size[0]) // stride[0] + 1\n",
        "        w_out = (input_size[1] + 2 * padding[1] - kernel_size[1]) // stride[1] + 1\n",
        "\n",
        "        # ê³±ì…ˆê³¼ ë§ì…ˆ ì—°ì‚°\n",
        "        multiplications = kernel_size[0] * kernel_size[1] * in_channels * out_channels * h_out * w_out\n",
        "        additions = (kernel_size[0] * kernel_size[1] * in_channels - 1) * out_channels * h_out * w_out\n",
        "\n",
        "        # Bias ì¶”ê°€ (optional)\n",
        "        bias_additions = out_channels * h_out * w_out\n",
        "\n",
        "        total_flops = multiplications + additions + bias_additions\n",
        "\n",
        "        return total_flops, (h_out, w_out)\n",
        "\n",
        "    @staticmethod\n",
        "    def linear_flops(in_features, out_features, batch_size=1):\n",
        "        \"\"\"Linear ë ˆì´ì–´ì˜ FLOPs ê³„ì‚°\n",
        "\n",
        "        FLOPs = 2 Ã— in_features Ã— out_features Ã— batch_size\n",
        "        \"\"\"\n",
        "        multiplications = in_features * out_features * batch_size\n",
        "        additions = (in_features - 1) * out_features * batch_size\n",
        "        bias_additions = out_features * batch_size\n",
        "\n",
        "        return multiplications + additions + bias_additions\n",
        "\n",
        "    @staticmethod\n",
        "    def attention_flops(seq_len, d_model, num_heads, batch_size=1):\n",
        "        \"\"\"Multi-Head Attentionì˜ FLOPs ê³„ì‚°\n",
        "\n",
        "        Q, K, V projection + Attention scores + Output projection\n",
        "        \"\"\"\n",
        "        d_head = d_model // num_heads\n",
        "\n",
        "        # Q, K, V projections\n",
        "        qkv_flops = 3 * FLOPsCalculator.linear_flops(d_model, d_model, batch_size * seq_len)\n",
        "\n",
        "        # Attention scores: Q @ K^T\n",
        "        attention_scores = 2 * batch_size * num_heads * seq_len * seq_len * d_head\n",
        "\n",
        "        # Softmax (approximated as seq_len operations per position)\n",
        "        softmax_flops = batch_size * num_heads * seq_len * seq_len * 5  # rough approximation\n",
        "\n",
        "        # Attention @ V\n",
        "        attention_output = 2 * batch_size * num_heads * seq_len * seq_len * d_head\n",
        "\n",
        "        # Output projection\n",
        "        output_projection = FLOPsCalculator.linear_flops(d_model, d_model, batch_size * seq_len)\n",
        "\n",
        "        total_flops = qkv_flops + attention_scores + softmax_flops + attention_output + output_projection\n",
        "\n",
        "        return total_flops"
      ],
      "metadata": {
        "id": "VACKlKiWG9Ge"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# FLOPs ê³„ì‚°\n",
        "def calculate_model_flops(model, input_size=(32, 32)):\n",
        "    \"\"\"ëª¨ë¸ì˜ ì´ FLOPs ê³„ì‚°\"\"\"\n",
        "    calculator = FLOPsCalculator()\n",
        "    total_flops = 0\n",
        "    layer_flops = {}\n",
        "\n",
        "    # Conv1: 3 -> 64\n",
        "    flops, output_size = calculator.conv2d_flops(3, 64, 3, input_size, padding=1)\n",
        "    layer_flops['conv1'] = flops\n",
        "    total_flops += flops\n",
        "    output_size = (output_size[0]//2, output_size[1]//2)  # After pooling\n",
        "\n",
        "    # Conv2: 64 -> 128\n",
        "    flops, output_size = calculator.conv2d_flops(64, 128, 3, output_size, padding=1)\n",
        "    layer_flops['conv2'] = flops\n",
        "    total_flops += flops\n",
        "    output_size = (output_size[0]//2, output_size[1]//2)  # After pooling\n",
        "\n",
        "    # Conv3: 128 -> 256\n",
        "    flops, output_size = calculator.conv2d_flops(128, 256, 3, output_size, padding=1)\n",
        "    layer_flops['conv3'] = flops\n",
        "    total_flops += flops\n",
        "    output_size = (output_size[0]//2, output_size[1]//2)  # After pooling\n",
        "\n",
        "    # FC1: 256*4*4 -> 512\n",
        "    flops = calculator.linear_flops(256 * 4 * 4, 512)\n",
        "    layer_flops['fc1'] = flops\n",
        "    total_flops += flops\n",
        "\n",
        "    # FC2: 512 -> 10\n",
        "    flops = calculator.linear_flops(512, 10)\n",
        "    layer_flops['fc2'] = flops\n",
        "    total_flops += flops\n",
        "\n",
        "    return total_flops, layer_flops\n",
        "\n",
        "model = SimpleCNN()\n",
        "total_flops, layer_flops = calculate_model_flops(model)\n",
        "\n",
        "print(f\"Total FLOPs: {total_flops:,}\")\n",
        "print(\"\\nLayer-wise FLOPs:\")\n",
        "for layer, flops in layer_flops.items():\n",
        "    print(f\"  {layer}: {flops:,} ({flops/total_flops*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3PuQJeKHDUW",
        "outputId": "b335cc33-2423-4ba4-de51-a9a7abcac067"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total FLOPs: 83,240,960\n",
            "\n",
            "Layer-wise FLOPs:\n",
            "  conv1: 3,538,944 (4.25%)\n",
            "  conv2: 37,748,736 (45.35%)\n",
            "  conv3: 37,748,736 (45.35%)\n",
            "  fc1: 4,194,304 (5.04%)\n",
            "  fc2: 10,240 (0.01%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from thop import profile, clever_format\n",
        "\n",
        "def profile_with_thop(model, input_size=(1, 3, 32, 32)):\n",
        "    \"\"\"THOPì„ ì‚¬ìš©í•œ FLOPs í”„ë¡œíŒŒì¼ë§\"\"\"\n",
        "    input_tensor = torch.randn(input_size)\n",
        "\n",
        "    # FLOPsì™€ Parameters ê³„ì‚°\n",
        "    flops, params = profile(model, inputs=(input_tensor,))\n",
        "\n",
        "    # ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜\n",
        "    flops, params = clever_format([flops, params], \"%.3f\")\n",
        "\n",
        "    print(f\"Model FLOPs: {flops}\")\n",
        "    print(f\"Model Parameters: {params}\")\n",
        "\n",
        "    return flops, params\n",
        "\n",
        "# ëª¨ë¸ í”„ë¡œíŒŒì¼ë§\n",
        "model = SimpleCNN()\n",
        "flops, params = profile_with_thop(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeFVlvqjHHGG",
        "outputId": "b9f77a11-654d-4aad-c5e4-4bc13e40f7f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "Model FLOPs: 41.620M\n",
            "Model Parameters: 2.474M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "\n",
        "def profile_with_fvcore(model, input_size=(1, 3, 32, 32)):\n",
        "    \"\"\"FVCoreë¥¼ ì‚¬ìš©í•œ ìƒì„¸ FLOPs ë¶„ì„\"\"\"\n",
        "    input_tensor = torch.randn(input_size)\n",
        "\n",
        "    # FLOPs ë¶„ì„\n",
        "    flops = FlopCountAnalysis(model, input_tensor)\n",
        "\n",
        "    # ì´ FLOPs\n",
        "    total_flops = flops.total()\n",
        "\n",
        "    # ë ˆì´ì–´ë³„ FLOPs\n",
        "    layer_flops = flops.by_module()\n",
        "\n",
        "    # ì—°ì‚° íƒ€ì…ë³„ FLOPs\n",
        "    op_flops = flops.by_operator()\n",
        "\n",
        "    print(f\"Total FLOPs: {total_flops:,}\")\n",
        "    print(\"\\nFLOPs by Layer:\")\n",
        "    for name, flops_count in layer_flops.items():\n",
        "        if flops_count > 0:\n",
        "            print(f\"  {name}: {flops_count:,}\")\n",
        "\n",
        "    print(\"\\nFLOPs by Operation Type:\")\n",
        "    for op, flops_count in op_flops.items():\n",
        "        if flops_count > 0:\n",
        "            print(f\"  {op}: {flops_count:,}\")\n",
        "\n",
        "    return total_flops, layer_flops, op_flops\n",
        "\n",
        "# FVCoreë¡œ ë¶„ì„\n",
        "total_flops, layer_flops, op_flops = profile_with_fvcore(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYhTkbIoHJUl",
        "outputId": "38c7eefb-52a9-4bdc-86cf-98b3e624fbae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total FLOPs: 41,620,480\n",
            "\n",
            "FLOPs by Layer:\n",
            "  : 41,620,480\n",
            "  conv1: 1,769,472\n",
            "  conv2: 18,874,368\n",
            "  conv3: 18,874,368\n",
            "  fc1: 2,097,152\n",
            "  fc2: 5,120\n",
            "\n",
            "FLOPs by Operation Type:\n",
            "  conv: 39,518,208\n",
            "  linear: 2,102,272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerProfiler:\n",
        "    \"\"\"Hookì„ ì‚¬ìš©í•œ ë ˆì´ì–´ë³„ ìƒì„¸ í”„ë¡œíŒŒì¼ë§\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layer_stats = {}\n",
        "\n",
        "    def hook_fn(self, module, input, output, name):\n",
        "        \"\"\"ê° ë ˆì´ì–´ì˜ ì…ì¶œë ¥ shape ë° FLOPs ê¸°ë¡\"\"\"\n",
        "        input_shape = input[0].shape if isinstance(input, tuple) else input.shape\n",
        "        output_shape = output.shape if hasattr(output, 'shape') else output[0].shape\n",
        "\n",
        "        self.layer_stats[name] = {\n",
        "            'input_shape': input_shape,\n",
        "            'output_shape': output_shape,\n",
        "            'module_type': module.__class__.__name__\n",
        "        }\n",
        "\n",
        "        # ê°„ë‹¨í•œ FLOPs ì¶”ì •\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            flops = self._conv_flops(module, output_shape)\n",
        "            self.layer_stats[name]['flops'] = flops\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            flops = self._linear_flops(module, output_shape)\n",
        "            self.layer_stats[name]['flops'] = flops\n",
        "\n",
        "    def _conv_flops(self, module, output_shape):\n",
        "        batch_size = output_shape[0]\n",
        "        out_h, out_w = output_shape[2], output_shape[3]\n",
        "        kernel_h, kernel_w = module.kernel_size\n",
        "        in_channels = module.in_channels\n",
        "        out_channels = module.out_channels\n",
        "\n",
        "        return 2 * batch_size * out_h * out_w * in_channels * out_channels * kernel_h * kernel_w\n",
        "\n",
        "    def _linear_flops(self, module, output_shape):\n",
        "        batch_size = output_shape[0]\n",
        "        return 2 * batch_size * module.in_features * module.out_features\n",
        "\n",
        "    def profile_model(self, model, input_tensor):\n",
        "        \"\"\"ëª¨ë¸ ì „ì²´ í”„ë¡œíŒŒì¼ë§\"\"\"\n",
        "        handles = []\n",
        "\n",
        "        # ê° ë ˆì´ì–´ì— hook ë“±ë¡\n",
        "        for name, module in model.named_modules():\n",
        "            if len(list(module.children())) == 0:  # Leaf modules only\n",
        "                handle = module.register_forward_hook(\n",
        "                    lambda m, i, o, n=name: self.hook_fn(m, i, o, n)\n",
        "                )\n",
        "                handles.append(handle)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = model(input_tensor)\n",
        "\n",
        "        # Hook ì œê±°\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "        return self.layer_stats\n",
        "\n",
        "# í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰\n",
        "profiler = LayerProfiler()\n",
        "input_tensor = torch.randn(1, 3, 32, 32)\n",
        "layer_stats = profiler.profile_model(model, input_tensor)\n",
        "\n",
        "print(\"Layer-wise Statistics:\")\n",
        "for name, stats in layer_stats.items():\n",
        "    print(f\"\\n{name} ({stats['module_type']}):\")\n",
        "    print(f\"  Input shape: {stats['input_shape']}\")\n",
        "    print(f\"  Output shape: {stats['output_shape']}\")\n",
        "    if 'flops' in stats:\n",
        "        print(f\"  FLOPs: {stats['flops']:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-RYw2HYHNWm",
        "outputId": "83ee3720-c3e7-4f68-bd6a-e3c584a4bdc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer-wise Statistics:\n",
            "\n",
            "conv1 (Conv2d):\n",
            "  Input shape: torch.Size([1, 3, 32, 32])\n",
            "  Output shape: torch.Size([1, 64, 32, 32])\n",
            "  FLOPs: 3,538,944\n",
            "\n",
            "pool (MaxPool2d):\n",
            "  Input shape: torch.Size([1, 256, 8, 8])\n",
            "  Output shape: torch.Size([1, 256, 4, 4])\n",
            "\n",
            "conv2 (Conv2d):\n",
            "  Input shape: torch.Size([1, 64, 16, 16])\n",
            "  Output shape: torch.Size([1, 128, 16, 16])\n",
            "  FLOPs: 37,748,736\n",
            "\n",
            "conv3 (Conv2d):\n",
            "  Input shape: torch.Size([1, 128, 8, 8])\n",
            "  Output shape: torch.Size([1, 256, 8, 8])\n",
            "  FLOPs: 37,748,736\n",
            "\n",
            "fc1 (Linear):\n",
            "  Input shape: torch.Size([1, 4096])\n",
            "  Output shape: torch.Size([1, 512])\n",
            "  FLOPs: 4,194,304\n",
            "\n",
            "fc2 (Linear):\n",
            "  Input shape: torch.Size([1, 512])\n",
            "  Output shape: torch.Size([1, 10])\n",
            "  FLOPs: 10,240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_throughput(model, batch_size, input_size=(3, 32, 32), num_iterations=100):\n",
        "    \"\"\"ëª¨ë¸ì˜ ì‹¤ì œ ì²˜ë¦¬ëŸ‰(throughput) ì¸¡ì •\"\"\"\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    dummy_input = torch.randn(batch_size, *input_size).cuda()\n",
        "    for _ in range(10):\n",
        "        _ = model(dummy_input)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # ì‹¤ì œ ì¸¡ì •\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        with torch.no_grad():\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # ì²˜ë¦¬ëŸ‰ ê³„ì‚°\n",
        "    elapsed_time = end_time - start_time\n",
        "    throughput = (batch_size * num_iterations) / elapsed_time\n",
        "\n",
        "    return throughput, elapsed_time\n",
        "\n",
        "# ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸\n",
        "batch_sizes = [1, 8, 16, 32, 64, 128]\n",
        "throughputs = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    try:\n",
        "        throughput, elapsed_time = measure_throughput(model, bs)\n",
        "        throughputs.append(throughput)\n",
        "        print(f\"Batch size {bs}: {throughput:.2f} samples/sec\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Batch size {bs}: OOM\")\n",
        "        throughputs.append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6e7yCEdHQDH",
        "outputId": "136fbaa5-ffa8-4fd2-c5db-2479f2413d6c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size 1: OOM\n",
            "Batch size 8: OOM\n",
            "Batch size 16: OOM\n",
            "Batch size 32: OOM\n",
            "Batch size 64: OOM\n",
            "Batch size 128: OOM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mfu_fixed(model, batch_size, input_size=(3, 32, 32), peak_flops=None):\n",
        "    \"\"\"\n",
        "    Model FLOPs Utilization ê³„ì‚° (Device ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „)\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch ëª¨ë¸\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "        input_size: ì…ë ¥ í¬ê¸° (C, H, W)\n",
        "        peak_flops: GPUì˜ ì´ë¡ ì  ìµœëŒ€ FLOPS\n",
        "\n",
        "    Returns:\n",
        "        dict: MFU ê³„ì‚° ê²°ê³¼\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Device í™•ì¸ ë° ì„¤ì •\n",
        "    device = next(model.parameters()).device\n",
        "    print(f\"ğŸ“ ëª¨ë¸ device: {device}\")\n",
        "\n",
        "    # 2. ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ê°™ì€ deviceì— ìƒì„±\n",
        "    input_tensor = torch.randn(batch_size, *input_size).to(device)\n",
        "    print(f\"ğŸ“ ì…ë ¥ í…ì„œ device: {input_tensor.device}\")\n",
        "\n",
        "    # 3. FLOPs ê³„ì‚°\n",
        "    try:\n",
        "        from fvcore.nn import FlopCountAnalysis\n",
        "        flops = FlopCountAnalysis(model, input_tensor).total()\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ FVCore ì—ëŸ¬: {e}\")\n",
        "        # ëŒ€ì²´ ë°©ë²• ì‚¬ìš©\n",
        "        from thop import profile\n",
        "        flops, _ = profile(model, inputs=(input_tensor,), verbose=False)\n",
        "\n",
        "    # 4. ì‹¤ì œ ì²˜ë¦¬ëŸ‰ ì¸¡ì • (ìˆ˜ì •ëœ ë²„ì „)\n",
        "    throughput, elapsed_time = measure_throughput_fixed(model, batch_size, input_size)\n",
        "\n",
        "    # 5. ì‹¤ì œ FLOPS ê³„ì‚°\n",
        "    actual_flops_per_sec = flops * throughput / batch_size\n",
        "\n",
        "    # 6. MFU ê³„ì‚°\n",
        "    if peak_flops:\n",
        "        mfu = (actual_flops_per_sec / peak_flops) * 100\n",
        "    else:\n",
        "        mfu = None\n",
        "\n",
        "    results = {\n",
        "        'batch_size': batch_size,\n",
        "        'model_flops': flops,\n",
        "        'throughput': throughput,\n",
        "        'actual_flops_per_sec': actual_flops_per_sec,\n",
        "        'mfu_percentage': mfu,\n",
        "        'device': str(device)\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def measure_throughput_fixed(model, batch_size, input_size=(3, 32, 32), num_iterations=100):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ì˜ ì‹¤ì œ ì²˜ë¦¬ëŸ‰(throughput) ì¸¡ì • (Device ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „)\n",
        "    \"\"\"\n",
        "    # ëª¨ë¸ì˜ device í™•ì¸\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up (ì¤‘ìš”!)\n",
        "    print(\"ğŸ”¥ Warming up...\")\n",
        "    dummy_input = torch.randn(batch_size, *input_size).to(device)  # device ì§€ì •\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    # GPU ë™ê¸°í™”\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # ì‹¤ì œ ì¸¡ì •\n",
        "    print(\"ğŸ“Š Measuring throughput...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_iterations):\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    # GPU ë™ê¸°í™”\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # ì²˜ë¦¬ëŸ‰ ê³„ì‚°\n",
        "    elapsed_time = end_time - start_time\n",
        "    throughput = (batch_size * num_iterations) / elapsed_time\n",
        "\n",
        "    print(f\"âœ… ì¸¡ì • ì™„ë£Œ: {throughput:.2f} samples/sec\")\n",
        "\n",
        "    return throughput, elapsed_time\n",
        "\n",
        "# ====================================\n",
        "# ğŸ¯ ì‹¤í–‰ ì˜ˆì œ\n",
        "# ====================================\n",
        "\n",
        "# ëª¨ë¸ ìƒì„± ë° GPU ì´ë™\n",
        "model = SimpleCNN()\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(\"âœ… GPU ì‚¬ìš© ì¤‘\")\n",
        "else:\n",
        "    print(\"âš ï¸ CPU ì‚¬ìš© ì¤‘ (GPU ê¶Œì¥)\")\n",
        "\n",
        "# MFU ì¸¡ì • (ìˆ˜ì •ëœ í•¨ìˆ˜ ì‚¬ìš©)\n",
        "results = calculate_mfu_fixed(model, batch_size=32, peak_flops=peak_flops)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ“Š MFU ì¸¡ì • ê²°ê³¼\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Device: {results['device']}\")\n",
        "print(f\"Model FLOPs: {results['model_flops']:,}\")\n",
        "print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
        "print(f\"Actual FLOPS: {results['actual_flops_per_sec']:.2e}\")\n",
        "if results['mfu_percentage']:\n",
        "    print(f\"MFU: {results['mfu_percentage']:.2f}%\")\n",
        "else:\n",
        "    print(\"MFU: N/A (peak_flops not provided)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2SvA4dfHSKf",
        "outputId": "9b79e2f3-7246-41c4-9f65-9d53331d999f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ CPU ì‚¬ìš© ì¤‘ (GPU ê¶Œì¥)\n",
            "ğŸ“ ëª¨ë¸ device: cpu\n",
            "ğŸ“ ì…ë ¥ í…ì„œ device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ Warming up...\n",
            "ğŸ“Š Measuring throughput...\n",
            "âœ… ì¸¡ì • ì™„ë£Œ: 350.42 samples/sec\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š MFU ì¸¡ì • ê²°ê³¼\n",
            "==================================================\n",
            "Device: cpu\n",
            "Model FLOPs: 1,331,855,360\n",
            "Throughput: 350.42 samples/sec\n",
            "Actual FLOPS: 1.46e+10\n",
            "MFU: N/A (peak_flops not provided)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_mfu_vs_batch_size(model, batch_sizes, peak_flops=None):\n",
        "    \"\"\"ë°°ì¹˜ í¬ê¸°ì— ë”°ë¥¸ MFU ë³€í™” ë¶„ì„\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        try:\n",
        "            result = calculate_mfu_fixed(model, bs, peak_flops=peak_flops)\n",
        "            results.append(result)\n",
        "            print(f\"Batch {bs}: MFU = {result['mfu_percentage']:.2f}%\" if result['mfu_percentage'] else f\"Batch {bs}: Completed\")\n",
        "        except RuntimeError:\n",
        "            print(f\"Batch {bs}: OOM\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ë¶„ì„ ì‹¤í–‰\n",
        "batch_sizes = [1, 4, 8, 16, 32, 64]\n",
        "mfu_results = analyze_mfu_vs_batch_size(model, batch_sizes, peak_flops)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "if mfu_results and any(r['mfu_percentage'] for r in mfu_results):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    valid_results = [r for r in mfu_results if r['mfu_percentage']]\n",
        "\n",
        "    batch_sizes_plot = [r['batch_size'] for r in valid_results]\n",
        "    mfu_values = [r['mfu_percentage'] for r in valid_results]\n",
        "\n",
        "    plt.plot(batch_sizes_plot, mfu_values, 'o-', linewidth=2, markersize=8)\n",
        "    plt.xlabel('Batch Size')\n",
        "    plt.ylabel('MFU (%)')\n",
        "    plt.title('Model FLOPs Utilization vs Batch Size')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xscale('log', base=2)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX265_1yHVte",
        "outputId": "edb1b075-769c-49e0-c361-e0fc860b826f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ ëª¨ë¸ device: cpu\n",
            "ğŸ“ ì…ë ¥ í…ì„œ device: cpu\n",
            "ğŸ”¥ Warming up...\n",
            "ğŸ“Š Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì¸¡ì • ì™„ë£Œ: 290.91 samples/sec\n",
            "Batch 1: Completed\n",
            "ğŸ“ ëª¨ë¸ device: cpu\n",
            "ğŸ“ ì…ë ¥ í…ì„œ device: cpu\n",
            "ğŸ”¥ Warming up...\n",
            "ğŸ“Š Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì¸¡ì • ì™„ë£Œ: 362.13 samples/sec\n",
            "Batch 4: Completed\n",
            "ğŸ“ ëª¨ë¸ device: cpu\n",
            "ğŸ“ ì…ë ¥ í…ì„œ device: cpu\n",
            "ğŸ”¥ Warming up...\n",
            "ğŸ“Š Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì¸¡ì • ì™„ë£Œ: 386.78 samples/sec\n",
            "Batch 8: Completed\n",
            "ğŸ“ ëª¨ë¸ device: cpu\n",
            "ğŸ“ ì…ë ¥ í…ì„œ device: cpu\n",
            "ğŸ”¥ Warming up...\n",
            "ğŸ“Š Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì¸¡ì • ì™„ë£Œ: 348.15 samples/sec\n",
            "Batch 16: Completed\n",
            "ğŸ“ ëª¨ë¸ device: cpu\n",
            "ğŸ“ ì…ë ¥ í…ì„œ device: cpu\n",
            "ğŸ”¥ Warming up...\n",
            "ğŸ“Š Measuring throughput...\n",
            "âœ… ì¸¡ì • ì™„ë£Œ: 369.10 samples/sec\n",
            "Batch 32: Completed\n",
            "ğŸ“ ëª¨ë¸ device: cpu\n",
            "ğŸ“ ì…ë ¥ í…ì„œ device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ Warming up...\n",
            "ğŸ“Š Measuring throughput...\n",
            "âœ… ì¸¡ì • ì™„ë£Œ: 419.00 samples/sec\n",
            "Batch 64: Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxmN3ZQ5HY7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zzkyyh96IUW0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}