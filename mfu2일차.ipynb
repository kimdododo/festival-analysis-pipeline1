{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODUIsdHtggMlJcgf2Ed6nb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdododo/festival-analysis-pipeline1/blob/main/mfu2%EC%9D%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPhYhMMkGtDl",
        "outputId": "505883be-ef07-45d8-a915-f2991a4e9a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from thop) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->thop) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->thop) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->thop) (3.0.3)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.15.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=94b2b8247383dc8189ecc832b4f317ce25fa12856e508a15aded85f066818b7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=1aca5131629c7c204a67cbff3d6cb1b34047d4e4a079b67ad5dbf1940b6322b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.5-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ptflops) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ptflops) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.3)\n",
            "Downloading ptflops-0.7.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.5\n",
            "Collecting torchprofile\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.12/dist-packages (from torchprofile) (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->torchprofile) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.4->torchprofile) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->torchprofile) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->torchprofile) (3.0.3)\n",
            "Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: torchprofile\n",
            "Successfully installed torchprofile-0.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install thop\n",
        "!pip install fvcore\n",
        "!pip install ptflops\n",
        "!pip install torchprofile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "cpsIEGjpGw0N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpu_info():\n",
        "    \"\"\"GPU 정보 및 이론적 최대 FLOPS 확인\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "        # GPU별 이론적 TFLOPS (대략적인 값)\n",
        "        theoretical_tflops = {\n",
        "            'A100': 312,  # FP16 Tensor Core\n",
        "            'V100': 125,  # FP16 Tensor Core\n",
        "            'T4': 65,     # FP16 Tensor Core\n",
        "            'P100': 21.2, # FP16\n",
        "        }\n",
        "\n",
        "        print(f\"GPU: {gpu_name}\")\n",
        "        print(f\"Memory: {gpu_memory:.2f} GB\")\n",
        "\n",
        "        for gpu_model, tflops in theoretical_tflops.items():\n",
        "            if gpu_model in gpu_name:\n",
        "                print(f\"Theoretical Peak Performance: {tflops} TFLOPS (FP16)\")\n",
        "                return tflops * 1e12  # Convert to FLOPS\n",
        "\n",
        "    return None\n",
        "\n",
        "peak_flops = get_gpu_info()\n",
        "peak_flops"
      ],
      "metadata": {
        "id": "AsbHrtTyG1MO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FLOPsCalculator:\n",
        "    \"\"\"각 레이어별 FLOPs를 수동으로 계산하는 클래스\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d_flops(in_channels, out_channels, kernel_size, input_size, stride=1, padding=0):\n",
        "        \"\"\"Conv2D 레이어의 FLOPs 계산\n",
        "\n",
        "        FLOPs = 2 × K² × C_in × C_out × H_out × W_out\n",
        "        \"\"\"\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "        if isinstance(stride, int):\n",
        "            stride = (stride, stride)\n",
        "        if isinstance(padding, int):\n",
        "            padding = (padding, padding)\n",
        "\n",
        "        h_out = (input_size[0] + 2 * padding[0] - kernel_size[0]) // stride[0] + 1\n",
        "        w_out = (input_size[1] + 2 * padding[1] - kernel_size[1]) // stride[1] + 1\n",
        "\n",
        "        # 곱셈과 덧셈 연산\n",
        "        multiplications = kernel_size[0] * kernel_size[1] * in_channels * out_channels * h_out * w_out\n",
        "        additions = (kernel_size[0] * kernel_size[1] * in_channels - 1) * out_channels * h_out * w_out\n",
        "\n",
        "        # Bias 추가 (optional)\n",
        "        bias_additions = out_channels * h_out * w_out\n",
        "\n",
        "        total_flops = multiplications + additions + bias_additions\n",
        "\n",
        "        return total_flops, (h_out, w_out)\n",
        "\n",
        "    @staticmethod\n",
        "    def linear_flops(in_features, out_features, batch_size=1):\n",
        "        \"\"\"Linear 레이어의 FLOPs 계산\n",
        "\n",
        "        FLOPs = 2 × in_features × out_features × batch_size\n",
        "        \"\"\"\n",
        "        multiplications = in_features * out_features * batch_size\n",
        "        additions = (in_features - 1) * out_features * batch_size\n",
        "        bias_additions = out_features * batch_size\n",
        "\n",
        "        return multiplications + additions + bias_additions\n",
        "\n",
        "    @staticmethod\n",
        "    def attention_flops(seq_len, d_model, num_heads, batch_size=1):\n",
        "        \"\"\"Multi-Head Attention의 FLOPs 계산\n",
        "\n",
        "        Q, K, V projection + Attention scores + Output projection\n",
        "        \"\"\"\n",
        "        d_head = d_model // num_heads\n",
        "\n",
        "        # Q, K, V projections\n",
        "        qkv_flops = 3 * FLOPsCalculator.linear_flops(d_model, d_model, batch_size * seq_len)\n",
        "\n",
        "        # Attention scores: Q @ K^T\n",
        "        attention_scores = 2 * batch_size * num_heads * seq_len * seq_len * d_head\n",
        "\n",
        "        # Softmax (approximated as seq_len operations per position)\n",
        "        softmax_flops = batch_size * num_heads * seq_len * seq_len * 5  # rough approximation\n",
        "\n",
        "        # Attention @ V\n",
        "        attention_output = 2 * batch_size * num_heads * seq_len * seq_len * d_head\n",
        "\n",
        "        # Output projection\n",
        "        output_projection = FLOPsCalculator.linear_flops(d_model, d_model, batch_size * seq_len)\n",
        "\n",
        "        total_flops = qkv_flops + attention_scores + softmax_flops + attention_output + output_projection\n",
        "\n",
        "        return total_flops"
      ],
      "metadata": {
        "id": "VACKlKiWG9Ge"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단한 CNN 모델 정의\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# FLOPs 계산\n",
        "def calculate_model_flops(model, input_size=(32, 32)):\n",
        "    \"\"\"모델의 총 FLOPs 계산\"\"\"\n",
        "    calculator = FLOPsCalculator()\n",
        "    total_flops = 0\n",
        "    layer_flops = {}\n",
        "\n",
        "    # Conv1: 3 -> 64\n",
        "    flops, output_size = calculator.conv2d_flops(3, 64, 3, input_size, padding=1)\n",
        "    layer_flops['conv1'] = flops\n",
        "    total_flops += flops\n",
        "    output_size = (output_size[0]//2, output_size[1]//2)  # After pooling\n",
        "\n",
        "    # Conv2: 64 -> 128\n",
        "    flops, output_size = calculator.conv2d_flops(64, 128, 3, output_size, padding=1)\n",
        "    layer_flops['conv2'] = flops\n",
        "    total_flops += flops\n",
        "    output_size = (output_size[0]//2, output_size[1]//2)  # After pooling\n",
        "\n",
        "    # Conv3: 128 -> 256\n",
        "    flops, output_size = calculator.conv2d_flops(128, 256, 3, output_size, padding=1)\n",
        "    layer_flops['conv3'] = flops\n",
        "    total_flops += flops\n",
        "    output_size = (output_size[0]//2, output_size[1]//2)  # After pooling\n",
        "\n",
        "    # FC1: 256*4*4 -> 512\n",
        "    flops = calculator.linear_flops(256 * 4 * 4, 512)\n",
        "    layer_flops['fc1'] = flops\n",
        "    total_flops += flops\n",
        "\n",
        "    # FC2: 512 -> 10\n",
        "    flops = calculator.linear_flops(512, 10)\n",
        "    layer_flops['fc2'] = flops\n",
        "    total_flops += flops\n",
        "\n",
        "    return total_flops, layer_flops\n",
        "\n",
        "model = SimpleCNN()\n",
        "total_flops, layer_flops = calculate_model_flops(model)\n",
        "\n",
        "print(f\"Total FLOPs: {total_flops:,}\")\n",
        "print(\"\\nLayer-wise FLOPs:\")\n",
        "for layer, flops in layer_flops.items():\n",
        "    print(f\"  {layer}: {flops:,} ({flops/total_flops*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3PuQJeKHDUW",
        "outputId": "b335cc33-2423-4ba4-de51-a9a7abcac067"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total FLOPs: 83,240,960\n",
            "\n",
            "Layer-wise FLOPs:\n",
            "  conv1: 3,538,944 (4.25%)\n",
            "  conv2: 37,748,736 (45.35%)\n",
            "  conv3: 37,748,736 (45.35%)\n",
            "  fc1: 4,194,304 (5.04%)\n",
            "  fc2: 10,240 (0.01%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from thop import profile, clever_format\n",
        "\n",
        "def profile_with_thop(model, input_size=(1, 3, 32, 32)):\n",
        "    \"\"\"THOP을 사용한 FLOPs 프로파일링\"\"\"\n",
        "    input_tensor = torch.randn(input_size)\n",
        "\n",
        "    # FLOPs와 Parameters 계산\n",
        "    flops, params = profile(model, inputs=(input_tensor,))\n",
        "\n",
        "    # 읽기 쉬운 형태로 변환\n",
        "    flops, params = clever_format([flops, params], \"%.3f\")\n",
        "\n",
        "    print(f\"Model FLOPs: {flops}\")\n",
        "    print(f\"Model Parameters: {params}\")\n",
        "\n",
        "    return flops, params\n",
        "\n",
        "# 모델 프로파일링\n",
        "model = SimpleCNN()\n",
        "flops, params = profile_with_thop(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeFVlvqjHHGG",
        "outputId": "b9f77a11-654d-4aad-c5e4-4bc13e40f7f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "Model FLOPs: 41.620M\n",
            "Model Parameters: 2.474M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, parameter_count\n",
        "\n",
        "def profile_with_fvcore(model, input_size=(1, 3, 32, 32)):\n",
        "    \"\"\"FVCore를 사용한 상세 FLOPs 분석\"\"\"\n",
        "    input_tensor = torch.randn(input_size)\n",
        "\n",
        "    # FLOPs 분석\n",
        "    flops = FlopCountAnalysis(model, input_tensor)\n",
        "\n",
        "    # 총 FLOPs\n",
        "    total_flops = flops.total()\n",
        "\n",
        "    # 레이어별 FLOPs\n",
        "    layer_flops = flops.by_module()\n",
        "\n",
        "    # 연산 타입별 FLOPs\n",
        "    op_flops = flops.by_operator()\n",
        "\n",
        "    print(f\"Total FLOPs: {total_flops:,}\")\n",
        "    print(\"\\nFLOPs by Layer:\")\n",
        "    for name, flops_count in layer_flops.items():\n",
        "        if flops_count > 0:\n",
        "            print(f\"  {name}: {flops_count:,}\")\n",
        "\n",
        "    print(\"\\nFLOPs by Operation Type:\")\n",
        "    for op, flops_count in op_flops.items():\n",
        "        if flops_count > 0:\n",
        "            print(f\"  {op}: {flops_count:,}\")\n",
        "\n",
        "    return total_flops, layer_flops, op_flops\n",
        "\n",
        "# FVCore로 분석\n",
        "total_flops, layer_flops, op_flops = profile_with_fvcore(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYhTkbIoHJUl",
        "outputId": "38c7eefb-52a9-4bdc-86cf-98b3e624fbae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total FLOPs: 41,620,480\n",
            "\n",
            "FLOPs by Layer:\n",
            "  : 41,620,480\n",
            "  conv1: 1,769,472\n",
            "  conv2: 18,874,368\n",
            "  conv3: 18,874,368\n",
            "  fc1: 2,097,152\n",
            "  fc2: 5,120\n",
            "\n",
            "FLOPs by Operation Type:\n",
            "  conv: 39,518,208\n",
            "  linear: 2,102,272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerProfiler:\n",
        "    \"\"\"Hook을 사용한 레이어별 상세 프로파일링\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layer_stats = {}\n",
        "\n",
        "    def hook_fn(self, module, input, output, name):\n",
        "        \"\"\"각 레이어의 입출력 shape 및 FLOPs 기록\"\"\"\n",
        "        input_shape = input[0].shape if isinstance(input, tuple) else input.shape\n",
        "        output_shape = output.shape if hasattr(output, 'shape') else output[0].shape\n",
        "\n",
        "        self.layer_stats[name] = {\n",
        "            'input_shape': input_shape,\n",
        "            'output_shape': output_shape,\n",
        "            'module_type': module.__class__.__name__\n",
        "        }\n",
        "\n",
        "        # 간단한 FLOPs 추정\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            flops = self._conv_flops(module, output_shape)\n",
        "            self.layer_stats[name]['flops'] = flops\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            flops = self._linear_flops(module, output_shape)\n",
        "            self.layer_stats[name]['flops'] = flops\n",
        "\n",
        "    def _conv_flops(self, module, output_shape):\n",
        "        batch_size = output_shape[0]\n",
        "        out_h, out_w = output_shape[2], output_shape[3]\n",
        "        kernel_h, kernel_w = module.kernel_size\n",
        "        in_channels = module.in_channels\n",
        "        out_channels = module.out_channels\n",
        "\n",
        "        return 2 * batch_size * out_h * out_w * in_channels * out_channels * kernel_h * kernel_w\n",
        "\n",
        "    def _linear_flops(self, module, output_shape):\n",
        "        batch_size = output_shape[0]\n",
        "        return 2 * batch_size * module.in_features * module.out_features\n",
        "\n",
        "    def profile_model(self, model, input_tensor):\n",
        "        \"\"\"모델 전체 프로파일링\"\"\"\n",
        "        handles = []\n",
        "\n",
        "        # 각 레이어에 hook 등록\n",
        "        for name, module in model.named_modules():\n",
        "            if len(list(module.children())) == 0:  # Leaf modules only\n",
        "                handle = module.register_forward_hook(\n",
        "                    lambda m, i, o, n=name: self.hook_fn(m, i, o, n)\n",
        "                )\n",
        "                handles.append(handle)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            _ = model(input_tensor)\n",
        "\n",
        "        # Hook 제거\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "        return self.layer_stats\n",
        "\n",
        "# 프로파일링 실행\n",
        "profiler = LayerProfiler()\n",
        "input_tensor = torch.randn(1, 3, 32, 32)\n",
        "layer_stats = profiler.profile_model(model, input_tensor)\n",
        "\n",
        "print(\"Layer-wise Statistics:\")\n",
        "for name, stats in layer_stats.items():\n",
        "    print(f\"\\n{name} ({stats['module_type']}):\")\n",
        "    print(f\"  Input shape: {stats['input_shape']}\")\n",
        "    print(f\"  Output shape: {stats['output_shape']}\")\n",
        "    if 'flops' in stats:\n",
        "        print(f\"  FLOPs: {stats['flops']:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-RYw2HYHNWm",
        "outputId": "83ee3720-c3e7-4f68-bd6a-e3c584a4bdc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer-wise Statistics:\n",
            "\n",
            "conv1 (Conv2d):\n",
            "  Input shape: torch.Size([1, 3, 32, 32])\n",
            "  Output shape: torch.Size([1, 64, 32, 32])\n",
            "  FLOPs: 3,538,944\n",
            "\n",
            "pool (MaxPool2d):\n",
            "  Input shape: torch.Size([1, 256, 8, 8])\n",
            "  Output shape: torch.Size([1, 256, 4, 4])\n",
            "\n",
            "conv2 (Conv2d):\n",
            "  Input shape: torch.Size([1, 64, 16, 16])\n",
            "  Output shape: torch.Size([1, 128, 16, 16])\n",
            "  FLOPs: 37,748,736\n",
            "\n",
            "conv3 (Conv2d):\n",
            "  Input shape: torch.Size([1, 128, 8, 8])\n",
            "  Output shape: torch.Size([1, 256, 8, 8])\n",
            "  FLOPs: 37,748,736\n",
            "\n",
            "fc1 (Linear):\n",
            "  Input shape: torch.Size([1, 4096])\n",
            "  Output shape: torch.Size([1, 512])\n",
            "  FLOPs: 4,194,304\n",
            "\n",
            "fc2 (Linear):\n",
            "  Input shape: torch.Size([1, 512])\n",
            "  Output shape: torch.Size([1, 10])\n",
            "  FLOPs: 10,240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_throughput(model, batch_size, input_size=(3, 32, 32), num_iterations=100):\n",
        "    \"\"\"모델의 실제 처리량(throughput) 측정\"\"\"\n",
        "    model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    dummy_input = torch.randn(batch_size, *input_size).cuda()\n",
        "    for _ in range(10):\n",
        "        _ = model(dummy_input)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # 실제 측정\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        with torch.no_grad():\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 처리량 계산\n",
        "    elapsed_time = end_time - start_time\n",
        "    throughput = (batch_size * num_iterations) / elapsed_time\n",
        "\n",
        "    return throughput, elapsed_time\n",
        "\n",
        "# 다양한 배치 크기로 테스트\n",
        "batch_sizes = [1, 8, 16, 32, 64, 128]\n",
        "throughputs = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    try:\n",
        "        throughput, elapsed_time = measure_throughput(model, bs)\n",
        "        throughputs.append(throughput)\n",
        "        print(f\"Batch size {bs}: {throughput:.2f} samples/sec\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Batch size {bs}: OOM\")\n",
        "        throughputs.append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6e7yCEdHQDH",
        "outputId": "136fbaa5-ffa8-4fd2-c5db-2479f2413d6c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size 1: OOM\n",
            "Batch size 8: OOM\n",
            "Batch size 16: OOM\n",
            "Batch size 32: OOM\n",
            "Batch size 64: OOM\n",
            "Batch size 128: OOM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mfu_fixed(model, batch_size, input_size=(3, 32, 32), peak_flops=None):\n",
        "    \"\"\"\n",
        "    Model FLOPs Utilization 계산 (Device 오류 수정 버전)\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch 모델\n",
        "        batch_size: 배치 크기\n",
        "        input_size: 입력 크기 (C, H, W)\n",
        "        peak_flops: GPU의 이론적 최대 FLOPS\n",
        "\n",
        "    Returns:\n",
        "        dict: MFU 계산 결과\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Device 확인 및 설정\n",
        "    device = next(model.parameters()).device\n",
        "    print(f\"📍 모델 device: {device}\")\n",
        "\n",
        "    # 2. 입력 텐서를 모델과 같은 device에 생성\n",
        "    input_tensor = torch.randn(batch_size, *input_size).to(device)\n",
        "    print(f\"📍 입력 텐서 device: {input_tensor.device}\")\n",
        "\n",
        "    # 3. FLOPs 계산\n",
        "    try:\n",
        "        from fvcore.nn import FlopCountAnalysis\n",
        "        flops = FlopCountAnalysis(model, input_tensor).total()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ FVCore 에러: {e}\")\n",
        "        # 대체 방법 사용\n",
        "        from thop import profile\n",
        "        flops, _ = profile(model, inputs=(input_tensor,), verbose=False)\n",
        "\n",
        "    # 4. 실제 처리량 측정 (수정된 버전)\n",
        "    throughput, elapsed_time = measure_throughput_fixed(model, batch_size, input_size)\n",
        "\n",
        "    # 5. 실제 FLOPS 계산\n",
        "    actual_flops_per_sec = flops * throughput / batch_size\n",
        "\n",
        "    # 6. MFU 계산\n",
        "    if peak_flops:\n",
        "        mfu = (actual_flops_per_sec / peak_flops) * 100\n",
        "    else:\n",
        "        mfu = None\n",
        "\n",
        "    results = {\n",
        "        'batch_size': batch_size,\n",
        "        'model_flops': flops,\n",
        "        'throughput': throughput,\n",
        "        'actual_flops_per_sec': actual_flops_per_sec,\n",
        "        'mfu_percentage': mfu,\n",
        "        'device': str(device)\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def measure_throughput_fixed(model, batch_size, input_size=(3, 32, 32), num_iterations=100):\n",
        "    \"\"\"\n",
        "    모델의 실제 처리량(throughput) 측정 (Device 오류 수정 버전)\n",
        "    \"\"\"\n",
        "    # 모델의 device 확인\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # 모델을 eval 모드로 설정\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up (중요!)\n",
        "    print(\"🔥 Warming up...\")\n",
        "    dummy_input = torch.randn(batch_size, *input_size).to(device)  # device 지정\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    # GPU 동기화\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # 실제 측정\n",
        "    print(\"📊 Measuring throughput...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_iterations):\n",
        "            _ = model(dummy_input)\n",
        "\n",
        "    # GPU 동기화\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # 처리량 계산\n",
        "    elapsed_time = end_time - start_time\n",
        "    throughput = (batch_size * num_iterations) / elapsed_time\n",
        "\n",
        "    print(f\"✅ 측정 완료: {throughput:.2f} samples/sec\")\n",
        "\n",
        "    return throughput, elapsed_time\n",
        "\n",
        "# ====================================\n",
        "# 🎯 실행 예제\n",
        "# ====================================\n",
        "\n",
        "# 모델 생성 및 GPU 이동\n",
        "model = SimpleCNN()\n",
        "\n",
        "# GPU 사용 가능 여부 확인\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(\"✅ GPU 사용 중\")\n",
        "else:\n",
        "    print(\"⚠️ CPU 사용 중 (GPU 권장)\")\n",
        "\n",
        "# MFU 측정 (수정된 함수 사용)\n",
        "results = calculate_mfu_fixed(model, batch_size=32, peak_flops=peak_flops)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📊 MFU 측정 결과\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Device: {results['device']}\")\n",
        "print(f\"Model FLOPs: {results['model_flops']:,}\")\n",
        "print(f\"Throughput: {results['throughput']:.2f} samples/sec\")\n",
        "print(f\"Actual FLOPS: {results['actual_flops_per_sec']:.2e}\")\n",
        "if results['mfu_percentage']:\n",
        "    print(f\"MFU: {results['mfu_percentage']:.2f}%\")\n",
        "else:\n",
        "    print(\"MFU: N/A (peak_flops not provided)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2SvA4dfHSKf",
        "outputId": "9b79e2f3-7246-41c4-9f65-9d53331d999f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ CPU 사용 중 (GPU 권장)\n",
            "📍 모델 device: cpu\n",
            "📍 입력 텐서 device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Warming up...\n",
            "📊 Measuring throughput...\n",
            "✅ 측정 완료: 350.42 samples/sec\n",
            "\n",
            "==================================================\n",
            "📊 MFU 측정 결과\n",
            "==================================================\n",
            "Device: cpu\n",
            "Model FLOPs: 1,331,855,360\n",
            "Throughput: 350.42 samples/sec\n",
            "Actual FLOPS: 1.46e+10\n",
            "MFU: N/A (peak_flops not provided)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_mfu_vs_batch_size(model, batch_sizes, peak_flops=None):\n",
        "    \"\"\"배치 크기에 따른 MFU 변화 분석\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for bs in batch_sizes:\n",
        "        try:\n",
        "            result = calculate_mfu_fixed(model, bs, peak_flops=peak_flops)\n",
        "            results.append(result)\n",
        "            print(f\"Batch {bs}: MFU = {result['mfu_percentage']:.2f}%\" if result['mfu_percentage'] else f\"Batch {bs}: Completed\")\n",
        "        except RuntimeError:\n",
        "            print(f\"Batch {bs}: OOM\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 분석 실행\n",
        "batch_sizes = [1, 4, 8, 16, 32, 64]\n",
        "mfu_results = analyze_mfu_vs_batch_size(model, batch_sizes, peak_flops)\n",
        "\n",
        "# 시각화\n",
        "if mfu_results and any(r['mfu_percentage'] for r in mfu_results):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    valid_results = [r for r in mfu_results if r['mfu_percentage']]\n",
        "\n",
        "    batch_sizes_plot = [r['batch_size'] for r in valid_results]\n",
        "    mfu_values = [r['mfu_percentage'] for r in valid_results]\n",
        "\n",
        "    plt.plot(batch_sizes_plot, mfu_values, 'o-', linewidth=2, markersize=8)\n",
        "    plt.xlabel('Batch Size')\n",
        "    plt.ylabel('MFU (%)')\n",
        "    plt.title('Model FLOPs Utilization vs Batch Size')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xscale('log', base=2)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX265_1yHVte",
        "outputId": "edb1b075-769c-49e0-c361-e0fc860b826f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📍 모델 device: cpu\n",
            "📍 입력 텐서 device: cpu\n",
            "🔥 Warming up...\n",
            "📊 Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 측정 완료: 290.91 samples/sec\n",
            "Batch 1: Completed\n",
            "📍 모델 device: cpu\n",
            "📍 입력 텐서 device: cpu\n",
            "🔥 Warming up...\n",
            "📊 Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 측정 완료: 362.13 samples/sec\n",
            "Batch 4: Completed\n",
            "📍 모델 device: cpu\n",
            "📍 입력 텐서 device: cpu\n",
            "🔥 Warming up...\n",
            "📊 Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 측정 완료: 386.78 samples/sec\n",
            "Batch 8: Completed\n",
            "📍 모델 device: cpu\n",
            "📍 입력 텐서 device: cpu\n",
            "🔥 Warming up...\n",
            "📊 Measuring throughput...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 측정 완료: 348.15 samples/sec\n",
            "Batch 16: Completed\n",
            "📍 모델 device: cpu\n",
            "📍 입력 텐서 device: cpu\n",
            "🔥 Warming up...\n",
            "📊 Measuring throughput...\n",
            "✅ 측정 완료: 369.10 samples/sec\n",
            "Batch 32: Completed\n",
            "📍 모델 device: cpu\n",
            "📍 입력 텐서 device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Warming up...\n",
            "📊 Measuring throughput...\n",
            "✅ 측정 완료: 419.00 samples/sec\n",
            "Batch 64: Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxmN3ZQ5HY7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zzkyyh96IUW0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}