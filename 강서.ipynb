{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf893ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [Target: 내수위] =====\n",
      "Linear          RMSE=0.000 MAE=0.000 R²=1.000\n",
      "Ridge           RMSE=0.000 MAE=0.000 R²=1.000\n",
      "Lasso           RMSE=0.003 MAE=0.001 R²=1.000\n",
      "RandomForest    RMSE=0.051 MAE=0.006 R²=0.999\n",
      "GradientBoosting RMSE=0.096 MAE=0.040 R²=0.995\n",
      "HistGB          RMSE=0.048 MAE=0.014 R²=0.999\n",
      "XGBoost         RMSE=0.043 MAE=0.012 R²=0.999\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4954\n",
      "[LightGBM] [Info] Number of data points in the train set: 51493, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.379232\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4941\n",
      "[LightGBM] [Info] Number of data points in the train set: 51493, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.372225\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4936\n",
      "[LightGBM] [Info] Number of data points in the train set: 51494, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.367549\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4943\n",
      "[LightGBM] [Info] Number of data points in the train set: 51494, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.375556\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4940\n",
      "[LightGBM] [Info] Number of data points in the train set: 51494, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.376909\n",
      "LightGBM        RMSE=0.038 MAE=0.009 R²=0.999\n",
      "CatBoost        RMSE=0.037 MAE=0.014 R²=0.999\n",
      "\n",
      "===== [Target: 외수위] =====\n",
      "Linear          RMSE=0.010 MAE=0.000 R²=1.000\n",
      "Ridge           RMSE=0.010 MAE=0.000 R²=1.000\n",
      "Lasso           RMSE=0.010 MAE=0.001 R²=1.000\n",
      "RandomForest    RMSE=0.039 MAE=0.006 R²=0.999\n",
      "GradientBoosting RMSE=0.057 MAE=0.029 R²=0.999\n",
      "HistGB          RMSE=0.046 MAE=0.015 R²=0.999\n",
      "XGBoost         RMSE=0.043 MAE=0.014 R²=0.999\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4954\n",
      "[LightGBM] [Info] Number of data points in the train set: 51493, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.330425\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4941\n",
      "[LightGBM] [Info] Number of data points in the train set: 51493, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.335387\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4936\n",
      "[LightGBM] [Info] Number of data points in the train set: 51494, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.322836\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4943\n",
      "[LightGBM] [Info] Number of data points in the train set: 51494, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.330216\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4940\n",
      "[LightGBM] [Info] Number of data points in the train set: 51494, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.334079\n",
      "LightGBM        RMSE=0.031 MAE=0.011 R²=1.000\n",
      "CatBoost        RMSE=0.031 MAE=0.016 R²=1.000\n",
      "\n",
      "[DONE] 모든 모델 비교 완료 → figs/compare_*.csv 확인\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =========================================================\n",
    "# 강서구 펌프장 내/외수위 예측 - 멀티모델 비교 베이스라인\n",
    "# =========================================================\n",
    "import os, re, platform, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# 0) 한글 폰트 자동 패치\n",
    "# ------------------------------\n",
    "def patch_korean_font():\n",
    "    sysname = platform.system().lower()\n",
    "    font_candidates = []\n",
    "    if \"windows\" in sysname:\n",
    "        font_candidates = [\"Malgun Gothic\", \"맑은 고딕\"]\n",
    "    elif \"darwin\" in sysname:\n",
    "        font_candidates = [\"AppleGothic\"]\n",
    "    else:\n",
    "        font_candidates = [\"NanumGothic\", \"DejaVu Sans\"]\n",
    "    for f in font_candidates:\n",
    "        try:\n",
    "            matplotlib.rcParams[\"font.family\"] = f\n",
    "            _ = plt.figure(); plt.plot([0,1],[0,1]); plt.title(\"폰트테스트\"); plt.close()\n",
    "            break\n",
    "        except: continue\n",
    "    matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "os.makedirs(\"figs\", exist_ok=True)\n",
    "patch_korean_font()\n",
    "\n",
    "# ------------------------------\n",
    "# 1) 파일 경로\n",
    "# ------------------------------\n",
    "PUMP_FILE   = \"1.csv\"\n",
    "HYDRO_FILE  = \"hydro_raw_2017_2025.csv\"\n",
    "RAIN_FILE   = \"rain_2017_2024_merged.csv\"\n",
    "SEWER_FILE  = \"sewer_2017_2024_merged.csv\"\n",
    "\n",
    "# ------------------------------\n",
    "# 2) 데이터 로드 & 병합\n",
    "# ------------------------------\n",
    "pump = pd.read_csv(PUMP_FILE)\n",
    "pump[\"timestamp\"] = pd.to_datetime(pump[\"timestamp\"], errors=\"coerce\")\n",
    "pump = pump.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "rain = pd.read_csv(RAIN_FILE)\n",
    "rain[\"timestamp\"] = pd.to_datetime(rain[\"일시\"], errors=\"coerce\") if \"일시\" in rain.columns else pd.to_datetime(rain[\"timestamp\"])\n",
    "rain = rain.rename(columns={\"강수량(mm)\":\"rain_mm\"})[[\"timestamp\",\"rain_mm\"]].sort_values(\"timestamp\")\n",
    "rain[\"rain_mm\"] = pd.to_numeric(rain[\"rain_mm\"], errors=\"coerce\")\n",
    "\n",
    "hydro = pd.read_csv(HYDRO_FILE)\n",
    "date_col = \"관측 일시\"\n",
    "hour_cols = [c for c in hydro.columns if re.fullmatch(r\"\\s*\\d{1,2}시\\s*\", str(c))]\n",
    "hydro_long = hydro.melt(id_vars=[date_col], value_vars=hour_cols, var_name=\"hour_k\", value_name=\"hydro_value\")\n",
    "hydro_long[date_col] = pd.to_datetime(hydro_long[date_col], errors=\"coerce\").dt.floor(\"D\")\n",
    "hydro_long[\"hour_num\"] = hydro_long[\"hour_k\"].str.extract(r\"(\\d{1,2})\").astype(int)\n",
    "hydro_long[\"timestamp\"] = hydro_long[date_col] + pd.to_timedelta(hydro_long[\"hour_num\"]-1, unit=\"h\")\n",
    "hydro_long[\"hydro_value\"] = pd.to_numeric(hydro_long[\"hydro_value\"], errors=\"coerce\")\n",
    "hydro_agg = hydro_long.groupby(\"timestamp\", as_index=False)[\"hydro_value\"].mean()\n",
    "\n",
    "sewer = pd.read_csv(SEWER_FILE)\n",
    "sewer[\"timestamp\"] = pd.to_datetime(sewer[\"timestamp\"], errors=\"coerce\")\n",
    "sewer_agg = sewer.groupby(\"timestamp\", as_index=False)[\"water_level\"].mean()\n",
    "\n",
    "def merge_asof_(left, right, tol=\"1H\"):\n",
    "    return pd.merge_asof(left.sort_values(\"timestamp\"),\n",
    "                         right.sort_values(\"timestamp\"),\n",
    "                         on=\"timestamp\", direction=\"backward\",\n",
    "                         tolerance=pd.Timedelta(tol))\n",
    "\n",
    "df = pump.copy()\n",
    "df = merge_asof_(df, rain, tol=\"1H\")\n",
    "df = merge_asof_(df, hydro_agg, tol=\"1H\")\n",
    "df = merge_asof_(df, sewer_agg, tol=\"30min\")\n",
    "\n",
    "# 결측치 보정\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[num_cols] = df[num_cols].ffill().bfill()\n",
    "for c in num_cols: df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "# ------------------------------\n",
    "# 3) 파생변수 생성\n",
    "# ------------------------------\n",
    "def add_time_features(df, base_cols=(\"rain_mm\",\"hydro_value\",\"water_level\",\"내수위\",\"외수위\")):\n",
    "    df = df.sort_values(\"timestamp\")\n",
    "    for col in base_cols:\n",
    "        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[f\"{col}_lag1\"]  = df[col].shift(1)\n",
    "            df[f\"{col}_lag2\"]  = df[col].shift(2)\n",
    "            df[f\"{col}_diff1\"] = df[col].diff()\n",
    "            df[f\"{col}_rm3\"]   = df[col].rolling(3).mean()\n",
    "            df[f\"{col}_rm6\"]   = df[col].rolling(6).mean()\n",
    "    return df\n",
    "\n",
    "df = add_time_features(df)\n",
    "for c in df.select_dtypes(include=[np.number]).columns:\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "# ------------------------------\n",
    "# 4) 모델 비교 함수\n",
    "# ------------------------------\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    \"Linear\": Pipeline([(\"sc\", StandardScaler()), (\"m\", LinearRegression())]),\n",
    "    \"Ridge\":  Pipeline([(\"sc\", StandardScaler()), (\"m\", Ridge(alpha=1.0))]),\n",
    "    \"Lasso\":  Pipeline([(\"sc\", StandardScaler()), (\"m\", Lasso(alpha=1e-3, max_iter=10000))]),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"HistGB\": HistGradientBoostingRegressor(max_depth=6, learning_rate=0.08, max_iter=500, random_state=42),\n",
    "}\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    models[\"XGBoost\"] = XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "except: pass\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    models[\"LightGBM\"] = LGBMRegressor(n_estimators=800, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "except: pass\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    models[\"CatBoost\"] = CatBoostRegressor(iterations=800, learning_rate=0.05, depth=6, loss_function=\"RMSE\", random_state=42, verbose=False)\n",
    "except: pass\n",
    "\n",
    "def evaluate_target(target):\n",
    "    drop_cols = [\"timestamp\",\"일자\",\"시각\",\"펌프장 주소\",\"위도\",\"경도\",\"데이터기준일자\", target]\n",
    "    feats = [c for c in df.columns if c not in drop_cols and df[c].dtype != \"O\"]\n",
    "    X, y = df[feats].copy(), df[target].copy()\n",
    "    mask = X.notna().all(axis=1) & y.notna()\n",
    "    X, y = X[mask], y[mask]\n",
    "\n",
    "    print(f\"\\n===== [Target: {target}] =====\")\n",
    "    results = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        oof_pred, oof_true = [], []\n",
    "        for tr_idx, va_idx in kf.split(X):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "            model.fit(X_tr, y_tr)\n",
    "            pred = model.predict(X_va)\n",
    "            oof_pred.extend(pred); oof_true.extend(y_va)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(oof_true, oof_pred))\n",
    "        mae  = mean_absolute_error(oof_true, oof_pred)\n",
    "        r2   = r2_score(oof_true, oof_pred)\n",
    "        results.append([name, rmse, mae, r2])\n",
    "        print(f\"{name:15s} RMSE={rmse:.3f} MAE={mae:.3f} R²={r2:.3f}\")\n",
    "\n",
    "    res_df = pd.DataFrame(results, columns=[\"model\",\"RMSE\",\"MAE\",\"R2\"]).sort_values(\"RMSE\")\n",
    "    res_df.to_csv(f\"figs/compare_{target}.csv\", index=False)\n",
    "    return res_df\n",
    "\n",
    "# ------------------------------\n",
    "# 5) 실행\n",
    "# ------------------------------\n",
    "for t in [\"내수위\",\"외수위\"]:\n",
    "    if t in df.columns:\n",
    "        evaluate_target(t)\n",
    "\n",
    "print(\"\\n[DONE] 모든 모델 비교 완료 → figs/compare_*.csv 확인\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "774cb383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 폰트 적용: Malgun Gothic\n",
      "\n",
      "==================== [타겟: 내수위] ====================\n",
      "[Linear    ] RMSE=0.0283  R²=0.99954\n",
      "[INFO] 중요도 스킵(Linear): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[INFO] SHAP 스킵(Linear): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[Ridge     ] RMSE=0.0303  R²=0.99947\n",
      "[INFO] 중요도 스킵(Ridge): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[INFO] SHAP 스킵(Ridge): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[Lasso     ] RMSE=0.0171  R²=0.99983\n",
      "[INFO] 중요도 스킵(Lasso): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[INFO] SHAP 스킵(Lasso): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[RandomForest] RMSE=0.2657  R²=0.95947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 994/1000 [00:45<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GBDT      ] RMSE=0.5646  R²=0.81706\n",
      "[HistGB    ] RMSE=0.4907  R²=0.86181\n",
      "[INFO] SHAP 스킵(HistGB): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 4.480353, while the model output was 4.625601. If this difference is acceptable you can set check_additivity=False to disable this check.\n",
      "[XGBoost   ] RMSE=0.4575  R²=0.87988\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5072\n",
      "[LightGBM] [Info] Number of data points in the train set: 29402, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 1.788364\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5025\n",
      "[LightGBM] [Info] Number of data points in the train set: 50786, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.954490\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5062\n",
      "[LightGBM] [Info] Number of data points in the train set: 57703, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.212853\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5058\n",
      "[LightGBM] [Info] Number of data points in the train set: 58312, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.382381\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5050\n",
      "[LightGBM] [Info] Number of data points in the train set: 61265, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.318891\n",
      "[LightGBM  ] RMSE=0.4149  R²=0.90119\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5075\n",
      "[LightGBM] [Info] Number of data points in the train set: 64367, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.374294\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5075\n",
      "[LightGBM] [Info] Number of data points in the train set: 64367, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 2.374294\n",
      "[INFO] SHAP 스킵(LightGBM): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was -0.159416, while the model output was -0.122128. If this difference is acceptable you can set check_additivity=False to disable this check.\n",
      "[CatBoost  ] RMSE=0.3152  R²=0.94299\n",
      "\n",
      "[최종 성능 비교 - 내수위]\n",
      "       Model     RMSE       R2\n",
      "       Lasso 0.017133 0.999832\n",
      "      Linear 0.028293 0.999541\n",
      "       Ridge 0.030339 0.999472\n",
      "RandomForest 0.265739 0.959475\n",
      "    CatBoost 0.315190 0.942989\n",
      "    LightGBM 0.414943 0.901192\n",
      "     XGBoost 0.457514 0.879878\n",
      "      HistGB 0.490722 0.861807\n",
      "        GBDT 0.564606 0.817061\n",
      "\n",
      "==================== [타겟: 외수위] ====================\n",
      "[Linear    ] RMSE=0.0130  R²=0.99994\n",
      "[INFO] 중요도 스킵(Linear): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[INFO] SHAP 스킵(Linear): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[Ridge     ] RMSE=0.0118  R²=0.99995\n",
      "[INFO] 중요도 스킵(Ridge): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[INFO] SHAP 스킵(Ridge): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[Lasso     ] RMSE=0.0213  R²=0.99984\n",
      "[INFO] 중요도 스킵(Lasso): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[INFO] SHAP 스킵(Lasso): Pipeline.__init__() got an unexpected keyword argument 'sc'\n",
      "[RandomForest] RMSE=0.1998  R²=0.98580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|===================| 993/1000 [00:50<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GBDT      ] RMSE=0.1961  R²=0.98631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 981/1000 [00:11<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HistGB    ] RMSE=0.2482  R²=0.97807\n",
      "[INFO] SHAP 스킵(HistGB): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 9.886080, while the model output was 9.468494. If this difference is acceptable you can set check_additivity=False to disable this check.\n",
      "[XGBoost   ] RMSE=0.3052  R²=0.96685\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5072\n",
      "[LightGBM] [Info] Number of data points in the train set: 29402, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 4.726457\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5016\n",
      "[LightGBM] [Info] Number of data points in the train set: 50786, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.515465\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5062\n",
      "[LightGBM] [Info] Number of data points in the train set: 57703, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.297030\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5056\n",
      "[LightGBM] [Info] Number of data points in the train set: 58312, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.547588\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5050\n",
      "[LightGBM] [Info] Number of data points in the train set: 61265, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.292333\n",
      "[LightGBM  ] RMSE=0.1670  R²=0.99008\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002228 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5075\n",
      "[LightGBM] [Info] Number of data points in the train set: 64367, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.330589\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5075\n",
      "[LightGBM] [Info] Number of data points in the train set: 64367, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 5.330589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|===================| 976/1000 [00:11<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SHAP 스킵(LightGBM): Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 3.068488, while the model output was 3.016279. If this difference is acceptable you can set check_additivity=False to disable this check.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 257\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m     oof_pred[va_idx] = m.predict(X.iloc[va_idx])\n\u001b[32m    260\u001b[39m rmse = \u001b[38;5;28mfloat\u001b[39m(np.sqrt(mean_squared_error(y, oof_pred)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\catboost\\core.py:5873\u001b[39m, in \u001b[36mCatBoostRegressor.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5872\u001b[39m     CatBoostRegressor._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5874\u001b[39m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5875\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5876\u001b[39m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\catboost\\core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\catboost\\core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =========================================================\n",
    "# 강서구 펌프장 내/외수위 예측 - E2E 멀티모델 비교 (원클릭)\n",
    "#   - 입력: 1.csv, hydro_raw_2017_2025.csv, rain_2017_2024_merged.csv, sewer_2017_2024_merged.csv\n",
    "#   - 출력: figs/<타겟>_<모델>_scatter.png, _importance.png, _shap_*.png, <타겟>_results.csv\n",
    "# =========================================================\n",
    "import os, re, platform, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# 0) 한글 폰트\n",
    "# ------------------------------\n",
    "def patch_korean_font():\n",
    "    sysname = platform.system().lower()\n",
    "    for f in ([\"Malgun Gothic\",\"맑은 고딕\"] if \"windows\" in sysname\n",
    "              else [\"AppleGothic\"] if \"darwin\" in sysname\n",
    "              else [\"NanumGothic\",\"DejaVu Sans\"]):\n",
    "        try:\n",
    "            matplotlib.rcParams[\"font.family\"] = f\n",
    "            _ = plt.figure(); plt.title(\"폰트테스트\"); plt.close()\n",
    "            print(f\"[INFO] 폰트 적용: {f}\"); break\n",
    "        except Exception: continue\n",
    "    matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "os.makedirs(\"figs\", exist_ok=True)\n",
    "patch_korean_font()\n",
    "\n",
    "# ------------------------------\n",
    "# 1) 파일 경로 & asof tolerances\n",
    "# ------------------------------\n",
    "PUMP_FILE   = \"1.csv\"\n",
    "HYDRO_FILE  = \"hydro_raw_2017_2025.csv\"\n",
    "RAIN_FILE   = \"rain_2017_2024_merged.csv\"\n",
    "SEWER_FILE  = \"sewer_2017_2024_merged.csv\"\n",
    "\n",
    "RAIN_TOL  = \"1H\"\n",
    "HYDRO_TOL = \"1H\"\n",
    "SEWER_TOL = \"30min\"\n",
    "\n",
    "# ------------------------------\n",
    "# 2) 데이터 로드/정규화\n",
    "# ------------------------------\n",
    "def load_pump(path=PUMP_FILE):\n",
    "    pump = pd.read_csv(path)\n",
    "    if \"timestamp\" in pump.columns:\n",
    "        pump[\"timestamp\"] = pd.to_datetime(pump[\"timestamp\"], errors=\"coerce\")\n",
    "    elif {\"일자\",\"시각\"} <= set(pump.columns):\n",
    "        ts_a = pd.to_datetime(pump[\"일자\"], errors=\"coerce\")\n",
    "        ts_b = pd.to_datetime(pump[\"시각\"], errors=\"coerce\")\n",
    "        pump[\"timestamp\"] = ts_a.combine_first(ts_b)\n",
    "    else:\n",
    "        raise KeyError(\"펌프 데이터에 'timestamp' 또는 ['일자','시각']가 필요합니다.\")\n",
    "    return pump.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "def load_rain(path=RAIN_FILE):\n",
    "    rain = pd.read_csv(path)\n",
    "    if \"timestamp\" not in rain.columns:\n",
    "        if \"일시\" in rain.columns:\n",
    "            rain[\"timestamp\"] = pd.to_datetime(rain[\"일시\"], errors=\"coerce\")\n",
    "        else:\n",
    "            raise KeyError(\"강우 데이터에 'timestamp' 또는 '일시'가 필요합니다.\")\n",
    "    else:\n",
    "        rain[\"timestamp\"] = pd.to_datetime(rain[\"timestamp\"], errors=\"coerce\")\n",
    "    if \"강수량(mm)\" in rain.columns:\n",
    "        rain.rename(columns={\"강수량(mm)\":\"rain_mm\"}, inplace=True)\n",
    "    if \"rain_mm\" not in rain.columns:\n",
    "        raise KeyError(\"강우 데이터에 '강수량(mm)' 또는 'rain_mm' 컬럼이 필요합니다.\")\n",
    "    rain[\"rain_mm\"] = pd.to_numeric(rain[\"rain_mm\"], errors=\"coerce\")\n",
    "    return rain[[\"timestamp\",\"rain_mm\"]].dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "\n",
    "def load_hydro(path=HYDRO_FILE):\n",
    "    hydro = pd.read_csv(path)\n",
    "    if \"관측 일시\" not in hydro.columns:\n",
    "        raise KeyError(\"수문 데이터에 '관측 일시' 컬럼이 필요합니다.\")\n",
    "    date_col = \"관측 일시\"\n",
    "    hour_cols = [c for c in hydro.columns if re.fullmatch(r\"\\s*\\d{1,2}시\\s*\", str(c))]\n",
    "    if not hour_cols:\n",
    "        raise KeyError(\"수문 데이터에서 시간 컬럼(예: '01시'~'24시')을 찾지 못했습니다.\")\n",
    "    long = hydro.melt(id_vars=[date_col], value_vars=hour_cols,\n",
    "                      var_name=\"hour_k\", value_name=\"hydro_value\")\n",
    "    long[date_col] = pd.to_datetime(long[date_col], errors=\"coerce\").dt.floor(\"D\")\n",
    "    long[\"hour_num\"] = long[\"hour_k\"].astype(str).str.extract(r\"(\\d{1,2})\").astype(int)\n",
    "    # 1~24시 → 0~23시(자정=0)\n",
    "    long[\"timestamp\"] = long[date_col] + pd.to_timedelta((long[\"hour_num\"]-1) % 24, unit=\"h\")\n",
    "    long[\"hydro_value\"] = pd.to_numeric(long[\"hydro_value\"], errors=\"coerce\")\n",
    "    agg = (long.dropna(subset=[\"timestamp\"])\n",
    "                .groupby(\"timestamp\", as_index=False)[\"hydro_value\"].mean())\n",
    "    return agg.sort_values(\"timestamp\")\n",
    "\n",
    "def load_sewer(path=SEWER_FILE):\n",
    "    sewer = pd.read_csv(path)\n",
    "    if not {\"timestamp\",\"water_level\"} <= set(sewer.columns):\n",
    "        raise KeyError(\"하수 데이터에 'timestamp','water_level' 필요\")\n",
    "    sewer[\"timestamp\"] = pd.to_datetime(sewer[\"timestamp\"], errors=\"coerce\")\n",
    "    sewer[\"water_level\"] = pd.to_numeric(sewer[\"water_level\"], errors=\"coerce\")\n",
    "    agg = sewer.groupby(\"timestamp\", as_index=False)[\"water_level\"].mean()\n",
    "    return agg.sort_values(\"timestamp\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3) 안전 asof merge\n",
    "# ------------------------------\n",
    "def merge_asof_(left, right, tol=\"1H\", on=\"timestamp\"):\n",
    "    if right is None or len(right)==0: return left\n",
    "    L = left.sort_values(on).copy()\n",
    "    R = right.sort_values(on).copy()\n",
    "    out = pd.merge_asof(L, R, on=on, direction=\"backward\",\n",
    "                        tolerance=pd.Timedelta(tol))\n",
    "    return out\n",
    "\n",
    "# ------------------------------\n",
    "# 4) 병합 & 기본 정리\n",
    "# ------------------------------\n",
    "pump  = load_pump()\n",
    "rain  = load_rain()\n",
    "hydro = load_hydro()\n",
    "sewer = load_sewer()\n",
    "\n",
    "df = pump.copy()\n",
    "df = merge_asof_(df, rain,  tol=RAIN_TOL)\n",
    "df = merge_asof_(df, hydro, tol=HYDRO_TOL)\n",
    "df = merge_asof_(df, sewer, tol=SEWER_TOL)\n",
    "\n",
    "# 수치형 보수(시계열 ffill→bfill→median)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df[num_cols] = df[num_cols].ffill().bfill()\n",
    "for c in num_cols: df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "# 문자열 컬럼 중 수치형으로 해석 가능한 것 변환\n",
    "cat_cols = {\"펌프장명\",\"펌프장 주소\",\"일자\",\"시각\"}\n",
    "for c in df.columns:\n",
    "    if c not in cat_cols and df[c].dtype == \"O\":\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5) 시계열 파생 (그룹=펌프장명, 누설 방지)\n",
    "# ------------------------------\n",
    "def add_time_features(_df, group_col=\"펌프장명\",\n",
    "                      base=(\"rain_mm\",\"hydro_value\",\"water_level\",\"내수위\",\"외수위\")):\n",
    "    df2 = _df.copy()\n",
    "    if group_col in df2.columns:\n",
    "        df2 = df2.sort_values([group_col,\"timestamp\"])\n",
    "        g = df2.groupby(group_col, group_keys=False)\n",
    "        for col in base:\n",
    "            if col in df2.columns and pd.api.types.is_numeric_dtype(df2[col]):\n",
    "                df2[f\"{col}_lag1\"]  = g[col].shift(1)\n",
    "                df2[f\"{col}_lag2\"]  = g[col].shift(2)\n",
    "                df2[f\"{col}_diff1\"] = g[col].diff()\n",
    "                df2[f\"{col}_rm3\"]   = g[col].rolling(3).mean().reset_index(level=0, drop=True)\n",
    "                df2[f\"{col}_rm6\"]   = g[col].rolling(6).mean().reset_index(level=0, drop=True)\n",
    "    else:\n",
    "        df2 = df2.sort_values(\"timestamp\")\n",
    "        for col in base:\n",
    "            if col in df2.columns and pd.api.types.is_numeric_dtype(df2[col]):\n",
    "                df2[f\"{col}_lag1\"]  = df2[col].shift(1)\n",
    "                df2[f\"{col}_lag2\"]  = df2[col].shift(2)\n",
    "                df2[f\"{col}_diff1\"] = df2[col].diff()\n",
    "                df2[f\"{col}_rm3\"]   = df2[col].rolling(3).mean()\n",
    "                df2[f\"{col}_rm6\"]   = df2[col].rolling(6).mean()\n",
    "    # 파생 NaN 보수\n",
    "    for c in df2.select_dtypes(include=[np.number]).columns:\n",
    "        df2[c] = df2[c].fillna(df2[c].median())\n",
    "    return df2\n",
    "\n",
    "df = add_time_features(df)\n",
    "\n",
    "# ------------------------------\n",
    "# 6) 모델 정의\n",
    "# ------------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    \"Linear\": Pipeline([(\"sc\", StandardScaler()), (\"m\", LinearRegression())]),\n",
    "    \"Ridge\":  Pipeline([(\"sc\", StandardScaler()), (\"m\", Ridge(alpha=1.0))]),\n",
    "    \"Lasso\":  Pipeline([(\"sc\", StandardScaler()), (\"m\", Lasso(alpha=0.001, max_iter=10000))]),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"GBDT\": GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "    \"HistGB\": HistGradientBoostingRegressor(max_iter=500, learning_rate=0.05, max_depth=6, random_state=42),\n",
    "}\n",
    "\n",
    "# 외부 부스팅 라이브러리(있으면 추가)\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    models[\"XGBoost\"] = XGBRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, tree_method=\"hist\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[INFO] XGBoost 미설치 → 스킵:\", e)\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    models[\"LightGBM\"] = LGBMRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=-1,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[INFO] LightGBM 미설치 → 스킵:\", e)\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    models[\"CatBoost\"] = CatBoostRegressor(\n",
    "        iterations=500, learning_rate=0.05, depth=6,\n",
    "        random_state=42, loss_function=\"RMSE\", verbose=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"[INFO] CatBoost 미설치 → 스킵:\", e)\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ------------------------------\n",
    "# 7) 타깃별 학습/검증/시각화\n",
    "# ------------------------------\n",
    "TARGETS = [t for t in [\"내수위\",\"외수위\"] if t in df.columns]\n",
    "DROP = {\"timestamp\",\"일자\",\"시각\",\"펌프장 주소\",\"위도\",\"경도\",\"데이터기준일자\"}\n",
    "\n",
    "for TARGET in TARGETS:\n",
    "    print(f\"\\n==================== [타겟: {TARGET}] ====================\")\n",
    "    feat_cols = [c for c in df.columns if c not in DROP|{TARGET} and df[c].dtype != \"O\"]\n",
    "    X = df[feat_cols].copy()\n",
    "    y = df[TARGET].copy()\n",
    "    mask = X.notna().all(axis=1) & y.notna()\n",
    "    X, y = X[mask], y[mask]\n",
    "\n",
    "    # GroupKFold(누설 차단) → 펌프장명 없으면 KFold\n",
    "    if \"펌프장명\" in df.columns:\n",
    "        groups = df.loc[X.index, \"펌프장명\"]\n",
    "        splitter = GroupKFold(n_splits=5).split(X, y, groups=groups)\n",
    "    else:\n",
    "        splitter = KFold(n_splits=5, shuffle=True, random_state=42).split(X)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # CV를 매 모델마다 재생성(한 번 소비하면 iterator 소진)\n",
    "        if \"펌프장명\" in df.columns:\n",
    "            splits = GroupKFold(n_splits=5).split(X, y, groups=df.loc[X.index, \"펌프장명\"])\n",
    "        else:\n",
    "            splits = KFold(n_splits=5, shuffle=True, random_state=42).split(X)\n",
    "\n",
    "        oof_pred = np.zeros(len(y))\n",
    "        for tr_idx, va_idx in splits:\n",
    "            m = model  # 파이프라인/모델은 새로 생성(파라미터 동일)\n",
    "            # 일부 모델은 상태가 남을 수 있어 동일 클래스로 재생성 권장\n",
    "            try:\n",
    "                m = model.__class__(**model.get_params())\n",
    "            except Exception:\n",
    "                pass\n",
    "            m.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
    "            oof_pred[va_idx] = m.predict(X.iloc[va_idx])\n",
    "\n",
    "        rmse = float(np.sqrt(mean_squared_error(y, oof_pred)))\n",
    "        r2   = float(r2_score(y, oof_pred))\n",
    "        results.append((name, rmse, r2))\n",
    "        print(f\"[{name:10s}] RMSE={rmse:.4f}  R²={r2:.5f}\")\n",
    "\n",
    "        # 회귀선 플롯 (OOF)\n",
    "        plt.figure(figsize=(5.2,4.2))\n",
    "        plt.scatter(y, oof_pred, s=8, alpha=0.45)\n",
    "        lo, hi = float(min(y.min(), oof_pred.min())), float(max(y.max(), oof_pred.max()))\n",
    "        plt.plot([lo, hi], [lo, hi], \"r--\", lw=1)\n",
    "        k = np.polyfit(y, oof_pred, 1)\n",
    "        xx = np.linspace(lo, hi, 100)\n",
    "        plt.plot(xx, k[0]*xx + k[1], \"g-\", lw=1.2)\n",
    "        plt.title(f\"{TARGET} - {name} (OOF)\\nRMSE={rmse:,.2f}  R²={r2:.5f}\")\n",
    "        plt.xlabel(\"실제값\"); plt.ylabel(\"예측값\")\n",
    "        plt.tight_layout(); plt.savefig(f\"figs/{TARGET}_{name}_scatter.png\", dpi=220); plt.close()\n",
    "\n",
    "        # 변수 중요도(트리/부스팅)\n",
    "        try:\n",
    "            fitted = model.__class__(**model.get_params())\n",
    "            fitted.fit(X, y)\n",
    "            if hasattr(fitted, \"feature_importances_\"):\n",
    "                imp = (pd.DataFrame({\"feature\": feat_cols, \"importance\": fitted.feature_importances_})\n",
    "                         .sort_values(\"importance\", ascending=False).head(20))\n",
    "                plt.figure(figsize=(8,6))\n",
    "                plt.barh(imp[\"feature\"][::-1], imp[\"importance\"][::-1])\n",
    "                plt.title(f\"{TARGET} - {name} 중요 변수 Top20 (FULL)\")\n",
    "                plt.tight_layout(); plt.savefig(f\"figs/{TARGET}_{name}_importance.png\", dpi=220); plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"[INFO] 중요도 스킵({name}):\", e)\n",
    "\n",
    "        # SHAP (가능한 모델만)\n",
    "        try:\n",
    "            import shap\n",
    "            nsample = min(1000, len(X))\n",
    "            Xs = X.sample(nsample, random_state=42)\n",
    "            fitted = model.__class__(**model.get_params()); fitted.fit(X, y)\n",
    "            explainer = shap.Explainer(fitted, Xs)\n",
    "            sh = explainer(Xs)\n",
    "\n",
    "            shap.summary_plot(sh, Xs, plot_type=\"bar\", show=False, max_display=20)\n",
    "            plt.title(f\"{TARGET} - {name} SHAP (bar)\")\n",
    "            plt.tight_layout(); plt.savefig(f\"figs/{TARGET}_{name}_shap_bar.png\", dpi=220); plt.close()\n",
    "\n",
    "            shap.summary_plot(sh, Xs, plot_type=\"dot\", show=False, max_display=20)\n",
    "            plt.title(f\"{TARGET} - {name} SHAP (dot)\")\n",
    "            plt.tight_layout(); plt.savefig(f\"figs/{TARGET}_{name}_shap_dot.png\", dpi=220); plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"[INFO] SHAP 스킵({name}):\", e)\n",
    "\n",
    "    # 결과 테이블\n",
    "    res = pd.DataFrame(results, columns=[\"Model\",\"RMSE\",\"R2\"]).sort_values(\"RMSE\")\n",
    "    print(f\"\\n[최종 성능 비교 - {TARGET}]\")\n",
    "    print(res.to_string(index=False))\n",
    "    res.to_csv(f\"figs/{TARGET}_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n[DONE] 전체 파이프라인 완료 → figs/ 폴더 결과 확인\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
